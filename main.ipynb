{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import gc\n",
    "from copy import deepcopy\n",
    "\n",
    "import numpy as np\n",
    "import transformers\n",
    "from transformers import BertModel, BertTokenizer\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from datasets import load_dataset, load_metric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_lists(datasets, args):\n",
    "    \"\"\"\"\"\"\n",
    "    label_lists = []\n",
    "    for task in args.tasks:\n",
    "        is_regression = task == \"stsb\"\n",
    "        if is_regression:\n",
    "            label_lists.append([None])\n",
    "        else:\n",
    "            label_lists.append(datasets[task].features[\"label\"].names)\n",
    "    return label_lists\n",
    "\n",
    "def get_num_labels(label_lists):\n",
    "    \"\"\" Get a list of number of labels for the tasks \"\"\"\n",
    "    return [len(label_list) for label_list in label_lists]\n",
    "\n",
    "def preprocess(datasets, tokenizer, args):\n",
    "    \n",
    "    def preprocess_function(examples):\n",
    "        inputs = (\n",
    "            (examples[sentence1_key],) if sentence2_key is None else (examples[sentence1_key], examples[sentence2_key])\n",
    "        )\n",
    "        result = tokenizer(*inputs, padding=args.padding, max_length=args.max_length, truncation=True)\n",
    "        return result\n",
    "\n",
    "    for task, dataset in datasets.items():\n",
    "        sentence1_key, sentence2_key = task_to_keys[task]\n",
    "        datasets[task] = datasets[task].map(preprocess_function, batched=True)\n",
    "    return datasets\n",
    "\n",
    "def get_split_datasets(datasets, split=\"train\", seed=None):\n",
    "    if split == \"train\":\n",
    "        split_datasets = {task:dataset[split].shuffle(seed=seed) for task, dataset in datasets.items()}\n",
    "    else:\n",
    "        split_datasets = {task:dataset[split] for task, dataset in datasets.items()}\n",
    "    return split_datasets\n",
    "\n",
    "def support_query_split(datasets, args):\n",
    "    support_datasets = {}\n",
    "    query_datasets   = {}\n",
    "    for task, dataset in datasets.items():\n",
    "        support_query_split    = dataset.train_test_split(test_size=args.query_size)\n",
    "        support_datasets[task] = support_query_split[\"train\"]\n",
    "        query_datasets[task]   = support_query_split[\"test\"]\n",
    "    return support_datasets, query_datasets\n",
    "\n",
    "def get_dataloaders(datasets, split, args):\n",
    "    dataloaders = []\n",
    "    for task, dataset in datasets.items():\n",
    "        num_rows = dataset.num_rows if args.num_rows == -1 else args.num_rows\n",
    "        all_input_ids      = np.zeros([num_rows, args.max_length])\n",
    "        all_attention_mask = np.zeros([num_rows, args.max_length])\n",
    "        all_token_type_ids = np.zeros([num_rows, args.max_length])\n",
    "        for i in range(num_rows):\n",
    "            features = dataset[i]\n",
    "            curr_len = len(features[\"attention_mask\"])\n",
    "            all_input_ids[i,:curr_len]      = features[\"input_ids\"]\n",
    "            all_attention_mask[i,:curr_len] = features[\"attention_mask\"]\n",
    "            all_token_type_ids[i,:curr_len] = features[\"token_type_ids\"]\n",
    "        all_input_ids      = torch.tensor(all_input_ids, dtype=torch.long)\n",
    "        all_attention_mask = torch.tensor(all_attention_mask, dtype=torch.long)\n",
    "        all_token_type_ids = torch.tensor(all_token_type_ids, dtype=torch.long)\n",
    "        all_label          = torch.tensor(dataset[:num_rows][\"label\"], dtype=torch.long)\n",
    "\n",
    "        data = TensorDataset(all_input_ids, all_attention_mask, all_token_type_ids, all_label)\n",
    "        if split in [\"train\", \"support\"]:\n",
    "            sampler    = RandomSampler(data)\n",
    "            dataloader = DataLoader(data, sampler=sampler, batch_size=args.train_batch_size)\n",
    "        else:\n",
    "            sampler    = SequentialSampler(data)\n",
    "            dataloader = DataLoader(data, sampler=sampler, batch_size=args.eval_batch_size)\n",
    "        dataloaders.append(dataloader)\n",
    "    return dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset glue (/Users/tttyuntian/.cache/huggingface/datasets/glue/rte/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)\n",
      "Reusing dataset glue (/Users/tttyuntian/.cache/huggingface/datasets/glue/cola/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing datasets.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fc167be7e2c405eb699c0c16541910e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descriptiâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/tttyuntian/.cache/huggingface/datasets/glue/rte/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4/cache-f2c5bda36f8fb644.arrow\n",
      "Loading cached processed dataset at /Users/tttyuntian/.cache/huggingface/datasets/glue/cola/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4/cache-37c9628462b4302e.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving training set.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nprint(\"Retrieving evaluation set.\")\\neval_datasets = get_split_datasets(datasets, \"validation\")\\neval_dataloaders = get_dataloaders(eval_datasets, \"validation\", args)\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TrainingArgs:\n",
    "    def __init__(self):\n",
    "        #self.tasks = [\"sst2\", \"qqp\", \"mnli\", \"qnli\"]\n",
    "        self.tasks = [\"rte\", \"cola\"]\n",
    "        self.task_shared = True\n",
    "        self.padding = True\n",
    "        self.max_length = 512\n",
    "        self.do_lower_case = True\n",
    "        self.seed = 1123\n",
    "        self.query_size = 0.2\n",
    "        self.num_rows = -1\n",
    "\n",
    "        # BERT hyperparameters\n",
    "        self.input_dim = 768\n",
    "\n",
    "        # MAML hyperparameters\n",
    "        self.num_update_steps = 1   #5\n",
    "        self.num_sample_tasks = 2   #8\n",
    "        self.outer_learning_rate = 5e-5\n",
    "        self.inner_learning_rate = 1e-3\n",
    "\n",
    "        # train/eval hyperparameters\n",
    "        self.num_train_epochs = 1   # 5\n",
    "        self.train_batch_size = 8\n",
    "        self.eval_batch_size = 8\n",
    "\n",
    "args = TrainingArgs()\n",
    "\n",
    "task_to_keys = {\n",
    "    \"cola\": (\"sentence\", None),\n",
    "    \"mnli\": (\"premise\", \"hypothesis\"),\n",
    "    \"mrpc\": (\"sentence1\", \"sentence2\"),\n",
    "    \"qnli\": (\"question\", \"sentence\"),\n",
    "    \"qqp\" : (\"question1\", \"question2\"),\n",
    "    \"rte\" : (\"sentence1\", \"sentence2\"),\n",
    "    \"sst2\": (\"sentence\", None),\n",
    "    \"stsb\": (\"sentence1\", \"sentence2\"),\n",
    "    \"wnli\": (\"sentence1\", \"sentence2\"),\n",
    "}\n",
    "\n",
    "\"\"\"\n",
    "task_cluster_dict = {\n",
    "    \"mrpc\": 0,\n",
    "    \"cola\": 1,\n",
    "    \"mnli\": 0,\n",
    "    \"sst2\": 1,\n",
    "    \"rte\" : 0,\n",
    "    \"wnli\": 0,\n",
    "    \"qqp\" : 0,\n",
    "    \"qnli\": 2,\n",
    "    \"stsb\": 3\n",
    "}\n",
    "task_clusters = [task_cluster_dict[task] for task in tasks] if args.task_shared else None\n",
    "\"\"\"\n",
    "\n",
    "print(\"Loading datasets.\")\n",
    "train_datasets      = {task:load_dataset(\"glue\", task, split=\"train\") for task in args.tasks}\n",
    "label_lists   = get_label_lists(train_datasets, args)\n",
    "num_labels    = get_num_labels(label_lists)\n",
    "\n",
    "print(\"Preprocessing datasets.\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case=args.do_lower_case)\n",
    "train_datasets  = preprocess(train_datasets, tokenizer, args)\n",
    "\n",
    "print(\"Retrieving training set.\")\n",
    "#train_datasets = get_split_datasets(datasets, \"train\", seed=args.seed)\n",
    "support_datasets, query_datasets = support_query_split(train_datasets, args)\n",
    "support_dataloaders = get_dataloaders(support_datasets, \"support\", args)\n",
    "query_dataloaders   = get_dataloaders(query_datasets, \"query\", args)\n",
    "\n",
    "\"\"\"\n",
    "print(\"Retrieving evaluation set.\")\n",
    "eval_datasets = get_split_datasets(datasets, \"validation\")\n",
    "eval_dataloaders = get_dataloaders(eval_datasets, \"validation\", args)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_datasets[\"rte\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/tttyuntian/.cache/huggingface/datasets/glue/rte/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4/cache-97268307b189f4f1.arrow\n",
      "Loading cached shuffled indices for dataset at /Users/tttyuntian/.cache/huggingface/datasets/glue/rte/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4/cache-dd8b4763388da9f4.arrow\n",
      "Loading cached processed dataset at /Users/tttyuntian/.cache/huggingface/datasets/glue/rte/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4/cache-81ffb0ff9b8b036a.arrow\n",
      "Loading cached shuffled indices for dataset at /Users/tttyuntian/.cache/huggingface/datasets/glue/rte/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4/cache-07d5c83c7b03c434.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label:  0\n",
      "num_rows:  1249\n",
      "label:  1\n",
      "num_rows:  1241\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 1, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_few_shot_dataset(args, train_datasets):\n",
    "    train_dataset = train_datasets[0]\n",
    "    ids = []\n",
    "    for label in range(num_labels[0]):\n",
    "        label_dataset = train_dataset.filter(lambda example:example[\"label\"]==label)\n",
    "        label_dataset = label_dataset.shuffle(seed=args.seed)\n",
    "        label_dataset = label_dataset.select(np.arange(args.k_shot))\n",
    "        ids.extend(label_dataset[\"idx\"])\n",
    "\n",
    "    train_dataset = [train_dataset.select(ids)]\n",
    "    return [train_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import CrossEntropyLoss, MSELoss\n",
    "\n",
    "\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, embedder, input_dim, n_classes, dropout=0.2):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.n_classes = n_classes\n",
    "        self.embedder = embedder\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.classifier = nn.Linear(input_dim, n_classes)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, token_type_ids, labels):\n",
    "        outputs = self.embedder(input_ids, attention_mask, token_type_ids)\n",
    "        pooled_output = outputs[1]\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "        \n",
    "        if self.n_classes == 1:\n",
    "            loss_function = MSELoss()\n",
    "            loss = loss_function(logits.view(-1), labels.view(-1))\n",
    "        else:\n",
    "            loss_function = CrossEntropyLoss()\n",
    "            loss = loss_function(logits.view(-1, self.n_classes), labels.view(-1))\n",
    "        return logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_steps(dataloaders):\n",
    "    return [len(dataloader.dataset) // (train_batch_size*(num_update_steps+1)) for dataloader in dataloaders]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Epoch 0\n",
      "0\n",
      "0 0\n",
      "0 1\n",
      "| inner_loss 0.559568 |\n",
      "1\n",
      "0 0\n",
      "0 1\n",
      "| inner_loss 0.811201 |\n"
     ]
    }
   ],
   "source": [
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "outer_optimizer = Adam(model.parameters(), lr=outer_learning_rate)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_steps_per_task = get_train_steps(dataloaders)\n",
    "\n",
    "classifiers = [Classifier(model, input_dim, num_labels[task_id]) for task_id in range(len(tasks))]\n",
    "\n",
    "for epoch_id in range(num_train_epochs):\n",
    "    print(\"Start Epoch {}\".format(epoch_id))\n",
    "    model.train()\n",
    "    sum_gradients = []\n",
    "    \n",
    "    # Get sample tasks based on Probability Proportional to Size (PPS)\n",
    "    sample_task_ids = []\n",
    "    for task_id in range(len(tasks)):\n",
    "        sample_task_ids += [task_id] * train_steps_per_task[task_id]\n",
    "    sample_task_ids = np.random.choice(sample_task_ids, len(sample_task_ids), replace = False)\n",
    "    \n",
    "    for sample_task_id, task_id in enumerate(sample_task_ids):\n",
    "        print(sample_task_id)\n",
    "        classifier = classifiers[task_id]\n",
    "        classifier.embedder = model\n",
    "        inner_optimizer = Adam(classifier.parameters(), lr=inner_learning_rate)\n",
    "        classifier.train()\n",
    "        \n",
    "        # Inner updates with support sets\n",
    "        all_loss = []\n",
    "        support_dataloader = support_dataloaders[task_id]    \n",
    "        for step_id in range(num_update_steps):\n",
    "            print(step_id)\n",
    "            batch = support_dataloader.next()\n",
    "            input_ids, attention_mask, token_type_ids, labels = tuple(t.to(device) for t in batch)\n",
    "            outputs = classifier(input_ids, attention_mask, token_type_ids, labels = labels)\n",
    "            loss = outputs[1]\n",
    "            loss.backward()\n",
    "            inner_optimizer.step()\n",
    "            inner_optimizer.zero_grad()\n",
    "            all_loss.append(loss.item())\n",
    "        print(\"| inner_loss {:8.6f} |\".format(np.mean(all_loss)))\n",
    "        \n",
    "        # Outer update with query set\n",
    "        query_batch = iter(query_dataloaders[task_id]).next()\n",
    "        q_input_ids, q_attention_mask, q_token_type_ids, q_labels = tuple(t.to(device) for t in query_batch)\n",
    "        q_outputs = classifier(q_input_ids, q_attention_mask, q_token_type_ids, labels=q_labels)\n",
    "        \n",
    "        # Compute the cumulative gradients of original BERT parameters\n",
    "        q_loss = q_outputs[1]\n",
    "        q_loss.backward()\n",
    "        classifier.to(torch.device(\"cpu\"))\n",
    "        for i, (name, params) in enumerate(classifier.named_parameters()):\n",
    "            if name.startswith(\"embedder\"):\n",
    "                if sample_task_id == 0:\n",
    "                    sum_gradients.append(deepcopy(params.grad))\n",
    "                else:\n",
    "                    sum_gradients[i] += deepcopy(params.grad)\n",
    "        \n",
    "    # Update BERT parameters after sampling num_sample_tasks\n",
    "    if sample_task_id % num_sample_tasks == (num_sample_tasks-1):\n",
    "        # Compute average gradient across tasks\n",
    "        for i in range(len(sum_gradients)):\n",
    "            sum_gradients[i] = sum_gradients[i] / num_sample_tasks\n",
    "\n",
    "        # Assign gradients for original BERT model and Update weights\n",
    "        for i, params in enumerate(model.parameters()):\n",
    "            params.grad = sum_gradients[i]\n",
    "\n",
    "        outer_optimizer.step()\n",
    "        outer_optimizer.zero_grad()\n",
    "        \n",
    "    #gc.collect()\n",
    "                \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "model.save_pretrained(\"../checkpoints/metabert_maml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           _i2:  3.4 KiB\n",
      "                          _i12:  3.2 KiB\n",
      "                           _i3:  3.2 KiB\n",
      "                          _i10:  3.2 KiB\n",
      "                           _i4:  3.2 KiB\n",
      "                           _i8:  3.2 KiB\n",
      "                           _i6:  3.2 KiB\n",
      "                          _i18:  2.2 KiB\n",
      "                           _i5:  2.2 KiB\n",
      "                           _i7:  2.2 KiB\n",
      "                           _i9:  2.2 KiB\n",
      "                          _i11:  2.2 KiB\n",
      "                          _i13:  2.2 KiB\n",
      "                 BertTokenizer:  2.0 KiB\n",
      "                    DataLoader:  1.4 KiB\n",
      "                  TrainingArgs:  1.4 KiB\n",
      "                     BertModel:  1.0 KiB\n",
      "                          Adam:  1.0 KiB\n",
      "                 TensorDataset:  1.0 KiB\n",
      "                 RandomSampler:  1.0 KiB\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "def sizeof_fmt(num, suffix='B'):\n",
    "    ''' by Fred Cirera,  https://stackoverflow.com/a/1094933/1870254, modified'''\n",
    "    for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n",
    "        if abs(num) < 1024.0:\n",
    "            return \"%3.1f %s%s\" % (num, unit, suffix)\n",
    "        num /= 1024.0\n",
    "    return \"%.1f %s%s\" % (num, 'Yi', suffix)\n",
    "\n",
    "for name, size in sorted(((name, sys.getsizeof(value)) for name, value in locals().items()),\n",
    "                         key= lambda x: -x[1])[:20]:\n",
    "    print(\"{:>30}: {:>8}\".format(name, sizeof_fmt(size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "params = dict(model.named_parameters())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[ 101, 2304, 7600,  ...,    0,    0,    0],\n",
       "         [ 101, 2006, 2238,  ...,    0,    0,    0],\n",
       "         [ 101, 1996, 7233,  ...,    0,    0,    0],\n",
       "         ...,\n",
       "         [ 101, 1037, 2235,  ...,    0,    0,    0],\n",
       "         [ 101, 2110, 2610,  ...,    0,    0,    0],\n",
       "         [ 101, 1996, 3587,  ...,    0,    0,    0]]),\n",
       " tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]]),\n",
       " tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]]),\n",
       " tensor([0, 0, 1, 0, 1, 1, 1, 1])]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(support_dataloaders[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "metabert",
   "language": "python",
   "name": "metabert"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
