{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import gc\n",
    "\n",
    "import numpy as np\n",
    "import transformers\n",
    "from transformers import BertModel, BertTokenizer\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from datasets import load_dataset, load_metric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_arguments():\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    #parser.add_argument(\"--exp_name\", type=str, default=\"exp1\")\n",
    "    parser.add_argument(\"--tasks\", type=list, \\\n",
    "                        default=[\"mrpc\", \"cola\", \"mnli\", \"sst2\", \"rte\", \"qqp\", \"qnli\", \"stsb\"])\n",
    "    parser.add_argument(\"--task_shared\", action=\"store_true\")\n",
    "    parser.add_argument(\"--seed\", type=int, default=1123)\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    print(\"input args:\\n\", json.dumps(vars(args), indent=4, separators=(\",\", \":\")))\n",
    "    return args\n",
    "\n",
    "def get_label_lists(datasets, tasks):\n",
    "    \"\"\"\"\"\"\n",
    "    label_lists = []\n",
    "    for task in tasks:\n",
    "        is_regression = task == \"stsb\"\n",
    "        if is_regression:\n",
    "            label_lists.append([None])\n",
    "        else:\n",
    "            label_lists.append(datasets[task][\"train\"].features[\"label\"].names)\n",
    "    return label_lists\n",
    "\n",
    "def get_num_labels(label_lists, task_clusters, task_shared):\n",
    "    \"\"\" Get a list of number of labels for the tasks \"\"\"\n",
    "    if task_shared:\n",
    "        num_labels = [len(label_list) for label_list in label_lists]\n",
    "    else:\n",
    "        cluster_num_labels = {0:3, 1:2, 2:2, 3:1}\n",
    "        num_labels = [cluster_num_labels[task_cluster] for task_cluster in task_clusters]\n",
    "    return num_labels\n",
    "\n",
    "def preprocess(datasets, tokenizer):\n",
    "    \n",
    "    def preprocess_function(examples):\n",
    "        args = (\n",
    "            (examples[sentence1_key],) if sentence2_key is None else (examples[sentence1_key], examples[sentence2_key])\n",
    "        )\n",
    "        result = tokenizer(*args, padding=padding, max_length=max_length, truncation=True)\n",
    "        return result\n",
    "\n",
    "    for task, dataset in datasets.items():\n",
    "        sentence1_key, sentence2_key = task_to_keys[task]\n",
    "        datasets[task] = datasets[task].map(preprocess_function, batched=True)\n",
    "    return datasets\n",
    "\n",
    "def get_split_datasets(datasets, split=\"train\", seed=None):\n",
    "    if split == \"train\":\n",
    "        split_datasets = {task:dataset[split].shuffle(seed=seed) for task, dataset in datasets.items()}\n",
    "    else:\n",
    "        split_datasets = {task:dataset[split] for task, dataset in datasets.items()}\n",
    "    return split_datasets\n",
    "\n",
    "def support_query_split(datasets):\n",
    "    support_datasets = {}\n",
    "    query_datasets   = {}\n",
    "    for task, dataset in datasets.items():\n",
    "        support_query_split    = dataset.train_test_split(test_size=query_size)\n",
    "        support_datasets[task] = support_query_split[\"train\"]\n",
    "        query_datasets[task]   = support_query_split[\"test\"]\n",
    "    return support_datasets, query_datasets\n",
    "\n",
    "def get_dataloaders(datasets, split=\"train\"):\n",
    "    dataloaders = []\n",
    "    for task, dataset in datasets.items():\n",
    "        all_input_ids      = np.zeros([dataset.num_rows, max_length])\n",
    "        all_attention_mask = np.zeros([dataset.num_rows, max_length])\n",
    "        all_token_type_ids = np.zeros([dataset.num_rows, max_length])\n",
    "        for i in range(dataset.num_rows):\n",
    "            features = dataset[i]\n",
    "            curr_len = len(features[\"attention_mask\"])\n",
    "            all_input_ids[i,:curr_len]      = features[\"input_ids\"]\n",
    "            all_attention_mask[i,:curr_len] = features[\"attention_mask\"]\n",
    "            all_token_type_ids[i,:curr_len] = features[\"token_type_ids\"]\n",
    "        all_input_ids      = torch.tensor(all_input_ids, dtype=torch.long)\n",
    "        all_attention_mask = torch.tensor(all_attention_mask, dtype=torch.long)\n",
    "        all_token_type_ids = torch.tensor(all_token_type_ids, dtype=torch.long)\n",
    "        all_label          = torch.tensor(dataset[:][\"label\"], dtype=torch.long)\n",
    "        \n",
    "        data = TensorDataset(all_input_ids, all_attention_mask, all_token_type_ids, all_label)\n",
    "        if split in [\"train\", \"support\"]:\n",
    "            sampler    = RandomSampler(data)\n",
    "            dataloader = DataLoader(data, sampler=sampler, batch_size=train_batch_size)\n",
    "        else:\n",
    "            sampler    = SequentialSampler(data)\n",
    "            dataloader = DataLoader(data, sampler=sampler, batch_size=eval_batch_size)\n",
    "        dataloaders.append(dataloader)\n",
    "    return dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset glue (/Users/tttyuntian/.cache/huggingface/datasets/glue/rte/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)\n",
      "Reusing dataset glue (/Users/tttyuntian/.cache/huggingface/datasets/glue/cola/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing datasets.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/tttyuntian/.cache/huggingface/datasets/glue/rte/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4/cache-8efda5aff4c45ac0.arrow\n",
      "Loading cached processed dataset at /Users/tttyuntian/.cache/huggingface/datasets/glue/rte/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4/cache-80d279cd989e639f.arrow\n",
      "Loading cached processed dataset at /Users/tttyuntian/.cache/huggingface/datasets/glue/rte/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4/cache-6e17d613e5b9f8b1.arrow\n",
      "Loading cached processed dataset at /Users/tttyuntian/.cache/huggingface/datasets/glue/cola/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4/cache-fd85b48d6ce18d90.arrow\n",
      "Loading cached processed dataset at /Users/tttyuntian/.cache/huggingface/datasets/glue/cola/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4/cache-44dc8d48946779cb.arrow\n",
      "Loading cached processed dataset at /Users/tttyuntian/.cache/huggingface/datasets/glue/cola/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4/cache-ea4ee1f1175dab28.arrow\n",
      "Loading cached shuffled indices for dataset at /Users/tttyuntian/.cache/huggingface/datasets/glue/rte/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4/cache-bbaad181a68a4b01.arrow\n",
      "Loading cached shuffled indices for dataset at /Users/tttyuntian/.cache/huggingface/datasets/glue/cola/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4/cache-c574fef5621faf7a.arrow\n",
      "Loading cached split indices for dataset at /Users/tttyuntian/.cache/huggingface/datasets/glue/cola/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4/cache-f34bd15906cc5d23.arrow and /Users/tttyuntian/.cache/huggingface/datasets/glue/cola/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4/cache-4d4d0bfb1d0b7829.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving training set.\n",
      "Retrieving evaluation set.\n"
     ]
    }
   ],
   "source": [
    "#tasks = [\"sst2\", \"qqp\", \"mnli\", \"qnli\"]\n",
    "tasks = [\"rte\", \"cola\"]\n",
    "task_shared = True\n",
    "padding = True\n",
    "max_length = 512\n",
    "do_lower_case = True\n",
    "seed = 1123\n",
    "query_size = 0.2\n",
    "\n",
    "# BERT hyperparameters\n",
    "input_dim = 768\n",
    "\n",
    "# MAML hyperparameters\n",
    "num_update_steps = 5\n",
    "num_sample_tasks = 8\n",
    "outer_learning_rate = 5e-5\n",
    "inner_learning_rate = 1e-3\n",
    "\n",
    "# train/eval hyperparameters\n",
    "num_train_epochs = 1\n",
    "train_batch_size = 32\n",
    "eval_batch_size = 32\n",
    "\n",
    "task_to_keys = {\n",
    "    \"cola\": (\"sentence\", None),\n",
    "    \"mnli\": (\"premise\", \"hypothesis\"),\n",
    "    \"mrpc\": (\"sentence1\", \"sentence2\"),\n",
    "    \"qnli\": (\"question\", \"sentence\"),\n",
    "    \"qqp\" : (\"question1\", \"question2\"),\n",
    "    \"rte\" : (\"sentence1\", \"sentence2\"),\n",
    "    \"sst2\": (\"sentence\", None),\n",
    "    \"stsb\": (\"sentence1\", \"sentence2\"),\n",
    "    \"wnli\": (\"sentence1\", \"sentence2\"),\n",
    "}\n",
    "\n",
    "task_cluster_dict = {\n",
    "    \"mrpc\": 0,\n",
    "    \"cola\": 1,\n",
    "    \"mnli\": 0,\n",
    "    \"sst2\": 1,\n",
    "    \"rte\" : 0,\n",
    "    \"wnli\": 0,\n",
    "    \"qqp\" : 0,\n",
    "    \"qnli\": 2,\n",
    "    \"stsb\": 3\n",
    "}\n",
    "task_clusters = [task_cluster_dict[task] for task in tasks] if task_shared else None\n",
    "\n",
    "print(\"Loading datasets.\")\n",
    "datasets      = {task:load_dataset(\"glue\", task) for task in tasks}\n",
    "label_lists   = get_label_lists(datasets, tasks)\n",
    "num_labels    = get_num_labels(label_lists, task_clusters, task_shared)\n",
    "\n",
    "print(\"Preprocessing datasets.\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case=do_lower_case)\n",
    "datasets  = preprocess(datasets, tokenizer)\n",
    "\n",
    "print(\"Retrieving training set.\")\n",
    "train_datasets = get_split_datasets(datasets, \"train\", seed=seed)\n",
    "support_datasets, query_datasets = support_query_split(train_datasets)\n",
    "support_dataloaders = get_dataloaders(support_datasets, \"support\")\n",
    "query_dataloaders   = get_dataloaders(query_datasets, \"query\")\n",
    "print(\"Retrieving evaluation set.\")\n",
    "eval_datasets = get_split_datasets(datasets, \"validation\")\n",
    "eval_dataloaders = get_dataloaders(eval_datasets, \"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import CrossEntropyLoss, MSELoss\n",
    "\n",
    "\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, embedder, input_dim, n_classes, dropout=0.2):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.n_classes = n_classes\n",
    "        self.embedder = embedder\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.classifier = nn.Linear(input_dim, n_classes)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, token_type_ids, labels):\n",
    "        outputs = self.embedder(input_ids, attention_mask, token_type_ids)\n",
    "        pooled_output = outputs[1]\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "        \n",
    "        if self.n_classes == 1:\n",
    "            loss_function = MSELoss()\n",
    "            loss = loss_function(logits.view(-1), labels.view(-1))\n",
    "        else:\n",
    "            loss_function = CrossEntropyLoss()\n",
    "            loss = loss_function(logits.view(-1, self.n_classes), labels.view(-1))\n",
    "        return logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "outer_optimizer = Adam(model.parameters(), lr=outer_learning_rate)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_steps_per_task = [dataset.num_rows // (train_batch_size*(num_update_steps+1)) for _, dataset in train_datasets.items()]\n",
    "\n",
    "sum_gradients = []\n",
    "classifiers = [Classifier(model, input_dim, num_labels[task_id]) for task_id in range(len(tasks))]\n",
    "\n",
    "for epoch_id in range(num_train_epochs):\n",
    "    print(\"Start Epoch {}\".format(epoch_id))\n",
    "    model.train()\n",
    "    \n",
    "    # Get sample tasks based on Probability Proportional to Size (PPS)\n",
    "    sample_task_ids = []\n",
    "    for task_id in range(len(tasks)):\n",
    "        sample_task_ids += [task_id] * train_steps_per_task[task_id]\n",
    "    sample_task_ids = np.random.choice(sample_task_ids, len(sample_task_ids), replace = False)\n",
    "    \n",
    "    for sample_task_id, task_id in enumerate(sample_task_ids):\n",
    "        classifier = classifiers[task_id]\n",
    "        classifier.embedder = model\n",
    "        inner_optimizer = Adam(classifier.parameters(), lr=inner_learning_rate)\n",
    "        classifier.train()\n",
    "        \n",
    "        # Inner updates with support sets\n",
    "        for step_id in range(num_update_steps):\n",
    "            all_loss = []\n",
    "            for inner_step, batch in enumerate(support_dataloaders[task_id]):\n",
    "                print(step_id, inner_step)\n",
    "                input_ids, attention_mask, token_type_ids, labels = tuple(t.to(device) for t in batch)\n",
    "                outputs = classifier(input_ids, attention_mask, token_type_ids, labels = labels)\n",
    "                loss = outputs[1]\n",
    "                loss.backward()\n",
    "                inner_optimizer.step()\n",
    "                inner_optimizer.zero_grad()\n",
    "                all_loss.append(loss.item())\n",
    "        print(\"| inner_loss {:8.6f} |\".format(np.mean(all_loss)))\n",
    "        \n",
    "        # Outer update with query set\n",
    "        query_batch = iter(query_dataloader[task_id]).next()\n",
    "        q_input_ids, q_attention_mask, q_token_type_ids, q_labels = tuple(t.to(device) for t in query_batch)\n",
    "        q_outputs = classifier(q_input_ids, q_attention_mask, q_token_type_ids, labels=q_labels)\n",
    "        \n",
    "        # Compute the cumulative gradients of original BERT parameters\n",
    "        q_loss = q_outputs[1]\n",
    "        q_loss.backward()\n",
    "        classifier.to(torch.device(\"cpu\"))\n",
    "        for i, (name, params) in enumerate(classifier.namsed_parameters()):\n",
    "            if name.startswith(\"embedder\"):\n",
    "                if sample_task_id == 0:\n",
    "                    sum_gradients.append(deepcopy(params.grad))\n",
    "                else:\n",
    "                    sum_gradients[i] += deepcopy(params.grad)\n",
    "        \n",
    "        # Update BERT parameters after sampling num_sample_tasks\n",
    "        if sample_task_id % num_sample_tasks == (num_sample_tasks-1):\n",
    "            # Compute average gradient across tasks\n",
    "            for i in range(len(sum_gradients)):\n",
    "                sum_gradients[i] = sum_gradients[i] / num_sample_tasks\n",
    "            \n",
    "            # Assign gradients for original BERT model and Update weights\n",
    "            for i, params in enumerate(model.parameters()):\n",
    "                params.grad = sum_gradients[i]\n",
    "            \n",
    "            outer_optimizer.step()\n",
    "            outer_optimizer.zero_grad()\n",
    "        \n",
    "        del sum_gradients\n",
    "        gc.collect()\n",
    "                \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "metabert",
   "language": "python",
   "name": "metabert"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
