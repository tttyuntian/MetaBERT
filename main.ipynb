{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import gc\n",
    "from copy import deepcopy\n",
    "\n",
    "import numpy as np\n",
    "import transformers\n",
    "from transformers import BertModel, BertTokenizer\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from datasets import load_dataset, load_metric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_lists(datasets, args):\n",
    "    \"\"\"\"\"\"\n",
    "    label_lists = []\n",
    "    for task in args.tasks:\n",
    "        is_regression = task == \"stsb\"\n",
    "        if is_regression:\n",
    "            label_lists.append([None])\n",
    "        else:\n",
    "            label_lists.append(datasets[task].features[\"label\"].names)\n",
    "    return label_lists\n",
    "\n",
    "def get_num_labels(label_lists):\n",
    "    \"\"\" Get a list of number of labels for the tasks \"\"\"\n",
    "    return [len(label_list) for label_list in label_lists]\n",
    "\n",
    "def preprocess(datasets, tokenizer, args):\n",
    "    \n",
    "    def preprocess_function(examples):\n",
    "        inputs = (\n",
    "            (examples[sentence1_key],) if sentence2_key is None else (examples[sentence1_key], examples[sentence2_key])\n",
    "        )\n",
    "        result = tokenizer(*inputs, padding=args.padding, max_length=args.max_length, truncation=True)\n",
    "        return result\n",
    "\n",
    "    for task, dataset in datasets.items():\n",
    "        sentence1_key, sentence2_key = task_to_keys[task]\n",
    "        datasets[task] = datasets[task].map(preprocess_function, batched=True)\n",
    "    return datasets\n",
    "\n",
    "def get_split_datasets(datasets, split=\"train\", seed=None):\n",
    "    if split == \"train\":\n",
    "        split_datasets = {task:dataset[split].shuffle(seed=seed) for task, dataset in datasets.items()}\n",
    "    else:\n",
    "        split_datasets = {task:dataset[split] for task, dataset in datasets.items()}\n",
    "    return split_datasets\n",
    "\n",
    "def support_query_split(datasets, args):\n",
    "    support_datasets = {}\n",
    "    query_datasets   = {}\n",
    "    for task, dataset in datasets.items():\n",
    "        support_query_split    = dataset.train_test_split(test_size=args.query_size)\n",
    "        support_datasets[task] = support_query_split[\"train\"]\n",
    "        query_datasets[task]   = support_query_split[\"test\"]\n",
    "    return support_datasets, query_datasets\n",
    "\n",
    "def get_dataloaders(datasets, split, args):\n",
    "    dataloaders = []\n",
    "    for task, dataset in datasets.items():\n",
    "        num_rows = dataset.num_rows if args.num_rows == -1 else args.num_rows\n",
    "        all_input_ids      = np.zeros([num_rows, args.max_length])\n",
    "        all_attention_mask = np.zeros([num_rows, args.max_length])\n",
    "        all_token_type_ids = np.zeros([num_rows, args.max_length])\n",
    "        for i in range(num_rows):\n",
    "            features = dataset[i]\n",
    "            curr_len = len(features[\"attention_mask\"])\n",
    "            all_input_ids[i,:curr_len]      = features[\"input_ids\"]\n",
    "            all_attention_mask[i,:curr_len] = features[\"attention_mask\"]\n",
    "            all_token_type_ids[i,:curr_len] = features[\"token_type_ids\"]\n",
    "        all_input_ids      = torch.tensor(all_input_ids, dtype=torch.long)\n",
    "        all_attention_mask = torch.tensor(all_attention_mask, dtype=torch.long)\n",
    "        all_token_type_ids = torch.tensor(all_token_type_ids, dtype=torch.long)\n",
    "        all_label          = torch.tensor(dataset[:num_rows][\"label\"], dtype=torch.long)\n",
    "\n",
    "        data = TensorDataset(all_input_ids, all_attention_mask, all_token_type_ids, all_label)\n",
    "        if split in [\"train\", \"support\"]:\n",
    "            sampler    = RandomSampler(data)\n",
    "            dataloader = DataLoader(data, sampler=sampler, batch_size=args.train_batch_size)\n",
    "        else:\n",
    "            sampler    = SequentialSampler(data)\n",
    "            dataloader = DataLoader(data, sampler=sampler, batch_size=args.eval_batch_size)\n",
    "        dataloaders.append(dataloader)\n",
    "    return dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class TrainingArgs:\n",
    "    def __init__(self):\n",
    "        #self.tasks = [\"sst2\", \"qqp\", \"mnli\", \"qnli\"]\n",
    "        self.tasks = [\"rte\", \"cola\"]\n",
    "        self.task_shared = True\n",
    "        self.padding = True\n",
    "        self.max_length = 512\n",
    "        self.do_lower_case = True\n",
    "        self.seed = 1123\n",
    "        self.query_size = 0.2\n",
    "        self.num_rows = -1\n",
    "\n",
    "        # BERT hyperparameters\n",
    "        self.input_dim = 768\n",
    "\n",
    "        # MAML hyperparameters\n",
    "        self.num_update_steps = 1   #5\n",
    "        self.num_sample_tasks = 2   #8\n",
    "        self.outer_learning_rate = 5e-5\n",
    "        self.inner_learning_rate = 1e-3\n",
    "\n",
    "        # train/eval hyperparameters\n",
    "        self.num_train_epochs = 1   # 5\n",
    "        self.train_batch_size = 8\n",
    "        self.eval_batch_size = 8\n",
    "\n",
    "args = TrainingArgs()\n",
    "\n",
    "task_to_keys = {\n",
    "    \"cola\": (\"sentence\", None),\n",
    "    \"mnli\": (\"premise\", \"hypothesis\"),\n",
    "    \"mrpc\": (\"sentence1\", \"sentence2\"),\n",
    "    \"qnli\": (\"question\", \"sentence\"),\n",
    "    \"qqp\" : (\"question1\", \"question2\"),\n",
    "    \"rte\" : (\"sentence1\", \"sentence2\"),\n",
    "    \"sst2\": (\"sentence\", None),\n",
    "    \"stsb\": (\"sentence1\", \"sentence2\"),\n",
    "    \"wnli\": (\"sentence1\", \"sentence2\"),\n",
    "}\n",
    "\n",
    "\"\"\"\n",
    "task_cluster_dict = {\n",
    "    \"mrpc\": 0,\n",
    "    \"cola\": 1,\n",
    "    \"mnli\": 0,\n",
    "    \"sst2\": 1,\n",
    "    \"rte\" : 0,\n",
    "    \"wnli\": 0,\n",
    "    \"qqp\" : 0,\n",
    "    \"qnli\": 2,\n",
    "    \"stsb\": 3\n",
    "}\n",
    "task_clusters = [task_cluster_dict[task] for task in tasks] if args.task_shared else None\n",
    "\"\"\"\n",
    "\n",
    "print(\"Loading datasets.\")\n",
    "train_datasets      = {task:load_dataset(\"glue\", task, split=\"train\") for task in args.tasks}\n",
    "label_lists   = get_label_lists(datasets, args)\n",
    "num_labels    = get_num_labels(label_lists)\n",
    "\n",
    "print(\"Preprocessing datasets.\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case=args.do_lower_case)\n",
    "train_datasets  = preprocess(train_datasets, tokenizer, args)\n",
    "\n",
    "print(\"Retrieving training set.\")\n",
    "#train_datasets = get_split_datasets(datasets, \"train\", seed=args.seed)\n",
    "support_datasets, query_datasets = support_query_split(train_datasets, args)\n",
    "support_dataloaders = get_dataloaders(support_datasets, \"support\", args)\n",
    "query_dataloaders   = get_dataloaders(query_datasets, \"query\", args)\n",
    "\n",
    "\"\"\"\n",
    "print(\"Retrieving evaluation set.\")\n",
    "eval_datasets = get_split_datasets(datasets, \"validation\")\n",
    "eval_dataloaders = get_dataloaders(eval_datasets, \"validation\", args)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import CrossEntropyLoss, MSELoss\n",
    "\n",
    "\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, embedder, input_dim, n_classes, dropout=0.2):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.n_classes = n_classes\n",
    "        self.embedder = embedder\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.classifier = nn.Linear(input_dim, n_classes)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, token_type_ids, labels):\n",
    "        outputs = self.embedder(input_ids, attention_mask, token_type_ids)\n",
    "        pooled_output = outputs[1]\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "        \n",
    "        if self.n_classes == 1:\n",
    "            loss_function = MSELoss()\n",
    "            loss = loss_function(logits.view(-1), labels.view(-1))\n",
    "        else:\n",
    "            loss_function = CrossEntropyLoss()\n",
    "            loss = loss_function(logits.view(-1, self.n_classes), labels.view(-1))\n",
    "        return logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_steps(dataloaders):\n",
    "    return [len(dataloader.dataset) // (train_batch_size*(num_update_steps+1)) for dataloader in dataloaders]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Epoch 0\n",
      "0\n",
      "0 0\n",
      "0 1\n",
      "| inner_loss 0.559568 |\n",
      "1\n",
      "0 0\n",
      "0 1\n",
      "| inner_loss 0.811201 |\n"
     ]
    }
   ],
   "source": [
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "outer_optimizer = Adam(model.parameters(), lr=outer_learning_rate)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_steps_per_task = get_train_steps(dataloaders)\n",
    "\n",
    "classifiers = [Classifier(model, input_dim, num_labels[task_id]) for task_id in range(len(tasks))]\n",
    "\n",
    "for epoch_id in range(num_train_epochs):\n",
    "    print(\"Start Epoch {}\".format(epoch_id))\n",
    "    model.train()\n",
    "    sum_gradients = []\n",
    "    \n",
    "    # Get sample tasks based on Probability Proportional to Size (PPS)\n",
    "    sample_task_ids = []\n",
    "    for task_id in range(len(tasks)):\n",
    "        sample_task_ids += [task_id] * train_steps_per_task[task_id]\n",
    "    sample_task_ids = np.random.choice(sample_task_ids, len(sample_task_ids), replace = False)\n",
    "    \n",
    "    for sample_task_id, task_id in enumerate(sample_task_ids):\n",
    "        print(sample_task_id)\n",
    "        classifier = classifiers[task_id]\n",
    "        classifier.embedder = model\n",
    "        inner_optimizer = Adam(classifier.parameters(), lr=inner_learning_rate)\n",
    "        classifier.train()\n",
    "        \n",
    "        # Inner updates with support sets\n",
    "        for step_id in range(num_update_steps):\n",
    "            all_loss = []\n",
    "            for inner_step, batch in enumerate(support_dataloaders[task_id]):\n",
    "                print(step_id, inner_step)\n",
    "                input_ids, attention_mask, token_type_ids, labels = tuple(t.to(device) for t in batch)\n",
    "                outputs = classifier(input_ids, attention_mask, token_type_ids, labels = labels)\n",
    "                loss = outputs[1]\n",
    "                loss.backward()\n",
    "                inner_optimizer.step()\n",
    "                inner_optimizer.zero_grad()\n",
    "                all_loss.append(loss.item())\n",
    "        print(\"| inner_loss {:8.6f} |\".format(np.mean(all_loss)))\n",
    "        \n",
    "        # Outer update with query set\n",
    "        query_batch = iter(query_dataloaders[task_id]).next()\n",
    "        q_input_ids, q_attention_mask, q_token_type_ids, q_labels = tuple(t.to(device) for t in query_batch)\n",
    "        q_outputs = classifier(q_input_ids, q_attention_mask, q_token_type_ids, labels=q_labels)\n",
    "        \n",
    "        # Compute the cumulative gradients of original BERT parameters\n",
    "        q_loss = q_outputs[1]\n",
    "        q_loss.backward()\n",
    "        classifier.to(torch.device(\"cpu\"))\n",
    "        for i, (name, params) in enumerate(classifier.named_parameters()):\n",
    "            if name.startswith(\"embedder\"):\n",
    "                if sample_task_id == 0:\n",
    "                    sum_gradients.append(deepcopy(params.grad))\n",
    "                else:\n",
    "                    sum_gradients[i] += deepcopy(params.grad)\n",
    "        \n",
    "    # Update BERT parameters after sampling num_sample_tasks\n",
    "    if sample_task_id % num_sample_tasks == (num_sample_tasks-1):\n",
    "        # Compute average gradient across tasks\n",
    "        for i in range(len(sum_gradients)):\n",
    "            sum_gradients[i] = sum_gradients[i] / num_sample_tasks\n",
    "\n",
    "        # Assign gradients for original BERT model and Update weights\n",
    "        for i, params in enumerate(model.parameters()):\n",
    "            params.grad = sum_gradients[i]\n",
    "\n",
    "        outer_optimizer.step()\n",
    "        outer_optimizer.zero_grad()\n",
    "        \n",
    "    #gc.collect()\n",
    "                \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "model.save_pretrained(\"../checkpoints/metabert_maml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           _i2:  3.4 KiB\n",
      "                          _i12:  3.2 KiB\n",
      "                           _i3:  3.2 KiB\n",
      "                          _i10:  3.2 KiB\n",
      "                           _i4:  3.2 KiB\n",
      "                           _i8:  3.2 KiB\n",
      "                           _i6:  3.2 KiB\n",
      "                          _i18:  2.2 KiB\n",
      "                           _i5:  2.2 KiB\n",
      "                           _i7:  2.2 KiB\n",
      "                           _i9:  2.2 KiB\n",
      "                          _i11:  2.2 KiB\n",
      "                          _i13:  2.2 KiB\n",
      "                 BertTokenizer:  2.0 KiB\n",
      "                    DataLoader:  1.4 KiB\n",
      "                  TrainingArgs:  1.4 KiB\n",
      "                     BertModel:  1.0 KiB\n",
      "                          Adam:  1.0 KiB\n",
      "                 TensorDataset:  1.0 KiB\n",
      "                 RandomSampler:  1.0 KiB\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "def sizeof_fmt(num, suffix='B'):\n",
    "    ''' by Fred Cirera,  https://stackoverflow.com/a/1094933/1870254, modified'''\n",
    "    for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n",
    "        if abs(num) < 1024.0:\n",
    "            return \"%3.1f %s%s\" % (num, unit, suffix)\n",
    "        num /= 1024.0\n",
    "    return \"%.1f %s%s\" % (num, 'Yi', suffix)\n",
    "\n",
    "for name, size in sorted(((name, sys.getsizeof(value)) for name, value in locals().items()),\n",
    "                         key= lambda x: -x[1])[:20]:\n",
    "    print(\"{:>30}: {:>8}\".format(name, sizeof_fmt(size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__name__ 57\n",
      "__doc__ 298\n",
      "__package__ 16\n",
      "__loader__ 16\n",
      "__spec__ 16\n",
      "__builtin__ 88\n",
      "__builtins__ 88\n",
      "_ih 272\n",
      "_oh 248\n",
      "_dh 80\n",
      "In 272\n",
      "Out 248\n",
      "get_ipython 72\n",
      "exit 64\n",
      "quit 64\n",
      "_ 215\n",
      "__ 264\n",
      "___ 248\n",
      "_i 587\n",
      "_ii 192\n",
      "_iii 1050\n",
      "_i1 380\n",
      "argparse 88\n",
      "json 88\n",
      "gc 88\n",
      "deepcopy 144\n",
      "np 88\n",
      "transformers 88\n",
      "BertModel 1064\n",
      "BertTokenizer 2008\n",
      "torch 88\n",
      "Adam 1064\n",
      "TensorDataset 1064\n",
      "DataLoader 1472\n",
      "RandomSampler 1064\n",
      "SequentialSampler 1064\n",
      "load_dataset 144\n",
      "load_metric 144\n",
      "_i2 3499\n",
      "_i3 3315\n",
      "_i4 3307\n",
      "get_label_lists 144\n",
      "get_num_labels 144\n",
      "preprocess 144\n",
      "get_split_datasets 144\n",
      "support_query_split 144\n",
      "get_dataloaders 144\n",
      "_i5 2291\n",
      "TrainingArgs 1472\n",
      "args 64\n",
      "task_to_keys 376\n",
      "datasets 248\n",
      "label_lists 104\n",
      "_i6 3292\n",
      "_i7 2291\n",
      "num_labels 104\n",
      "tokenizer 64\n",
      "_i8 3296\n",
      "_i9 2291\n",
      "train_datasets 248\n",
      "support_datasets 248\n",
      "query_datasets 248\n",
      "_i10 3311\n",
      "_i11 2291\n",
      "_i12 3321\n",
      "_i13 2291\n",
      "support_dataloaders 104\n",
      "query_dataloaders 104\n",
      "_i14 57\n",
      "_14 248\n",
      "_i15 132\n",
      "_i16 220\n",
      "task 53\n",
      "dataset 264\n",
      "_i17 65\n",
      "_17 264\n",
      "_i18 2299\n",
      "_18 215\n",
      "_i19 587\n",
      "sys 88\n",
      "sizeof_fmt 144\n",
      "name 62\n",
      "size 28\n",
      "_i20 1050\n",
      "nn 88\n",
      "CrossEntropyLoss 1064\n",
      "MSELoss 1064\n",
      "Classifier 1064\n",
      "_i21 192\n",
      "get_train_steps 144\n",
      "_i22 587\n",
      "_i23 123\n"
     ]
    }
   ],
   "source": [
    "for var, obj in list(locals().items()):\n",
    "    print(var, sys.getsizeof(obj))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "metabert",
   "language": "python",
   "name": "metabert"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
